---
layout: post
title:  DeepLearning笔记(14)——NLP
date:   2018-05-09 19:00:00 +0800
---

* TOC
{:toc}

## 1. 词汇表示

（1）One-hot

假设有一个词汇表，V=[a, arron, ..., zulu, UNK]，则可以用 $O_i$ 来表示一个词，$O$ 即 One-hot，$i$ 表示这个词在词汇表中的位置。假设词汇表的长度为10000，则每个词都是一个10000维的向量。

这样的表示存在着一个很大的问题，就是把每个词都孤立起来了，从而导致算法对相关词汇的泛华能力不够。例如模型已经学到了一个句子：

I want a glass of orange ( juice ).

但是如果遇到另一个类似的句子：

I want a glass of apple ( ? ).

很明显这里很有可能也是 juice，但是由于模型不知道 orange 和 apple 的相似关系，因此难以做出判断。这是由于 one-hot 的表示方法，导致任意两个词向量的内积都是0，因此无法得知词与词之间的关系。

（2）Embedding

Embedding 是使用词的特征来表示一个词。例如：

<table>
    <thead>
        <tr>
            <th></th>
            <th>man</th>
            <th>woman</th>
            <th>king</th>
            <th>queen</th>
            <th>apple</th>
            <th>orange</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>gender</td>
            <td>-1</td>
            <td>1</td>
            <td>-0.95</td>
            <td>0.97</td>
            <td>0.00</td>
            <td>0.01</td>
        </tr>
        <tr>
            <td>royal</td>
            <td>0.01</td>
            <td>0.02</td>
            <td>0.93</td>
            <td>0.95</td>
            <td>-0.01</td>
            <td>0.00</td>
        </tr>
        <tr>
            <td>age</td>
            <td>0.03</td>
            <td>0.02</td>
            <td>0.7</td>
            <td>0.69</td>
            <td>0.03</td>
            <td>-0.02</td>
        </tr>
        <tr>
            <td>food</td>
            <td>0.09</td>
            <td>0.01</td>
            <td>0.02</td>
            <td>0.01</td>
            <td>0.95</td>
            <td>0.97</td>
        </tr>
        <tr>
            <td>⋮</td>
            <td>⋮</td>
            <td>⋮</td>
            <td>⋮</td>
            <td>⋮</td>
            <td>⋮</td>
            <td>⋮</td>
        </tr>
    </tbody>
</table>

假设一共有300个特征，则每个词可以用一个300维的向量来表示。将这些词在向量空间中进行可视化，则可以发现如下的分布（使用t-SNE算法降维可视化）：

![]({{site.baseurl}}/images/deeplearning/14-1.png)

## 2. 词嵌入的特性

词嵌入（Word Embedding）的一个很重要特性就是类比推理。例如下面的词汇表：

<table>
    <thead>
        <tr>
            <th></th>
            <th>man</th>
            <th>woman</th>
            <th>king</th>
            <th>queen</th>
            <th>apple</th>
            <th>orange</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>gender</td>
            <td>-1</td>
            <td>1</td>
            <td>-0.95</td>
            <td>0.97</td>
            <td>0.00</td>
            <td>0.01</td>
        </tr>
        <tr>
            <td>royal</td>
            <td>0.01</td>
            <td>0.02</td>
            <td>0.93</td>
            <td>0.95</td>
            <td>-0.01</td>
            <td>0.00</td>
        </tr>
        <tr>
            <td>age</td>
            <td>0.03</td>
            <td>0.02</td>
            <td>0.7</td>
            <td>0.69</td>
            <td>0.03</td>
            <td>-0.02</td>
        </tr>
        <tr>
            <td>food</td>
            <td>0.09</td>
            <td>0.01</td>
            <td>0.02</td>
            <td>0.01</td>
            <td>0.95</td>
            <td>0.97</td>
        </tr>
    </tbody>
</table>

假设每个单词用一个四维的向量来表示。用 $e_{word}$ 来表示某一个词的向量。则可以发现man之于woman正如king之于queen。即：

$$e_{man}-e_{woman} \approx e_{king}-e_{queen}$$

在已知 $e_{man}$，$e_{woman}$ 和 $e_{king}$ 的情况下，可以利用余弦相似度求得 $e_w$，使得：

$$e_{man}-e_{woman} \approx e_{king}-e_w$$

对于两个向量 $u$ 和 $v$，则余弦相似度为：

$$sim(u,v)=\frac{u^Tv}{\lvert\lvert u \rvert\rvert \cdot \lvert\lvert v \rvert\rvert}$$

因此通过计算 $sim(e_w,e_{king}-e_{man}+e_{woman})$ 即可得到最恰当的 $e_w$。

## 3. 嵌入矩阵

假设词库中有10000个词，每个词的embedding是一个300维的向量。则整个词库的嵌入矩阵表示为：

$$
E=\begin{bmatrix}
\mid & & \mid & & \mid & \mid \\
a & \cdots & orange & \cdots & zulu & \langle UNK \rangle \\
\mid & & \mid & & \mid & \mid
\end{bmatrix}
$$

$E$ 的大小为 $300\times10000$。

令 $e_w$ 表示单词 $w$ 的embedding向量（300维），$o_w$ 表示单词 $w$ 的one-hot表示的向量（10000维），则：

$$E \cdot o_w=e_w$$

## 4. Word2Vec

这部分主要讲解Word2Vec的skip-gram模型（另外一种模型是CBOW）。

假设我们有这样一个句子：I want a glass of orange juice to go along with my cereal. 在skip-gram模型中，需要从句子中抽取出上下文（context）和目标词（target）的配对，从而构建出一个监督学习。

从句子中选取一个词，然后这个词的前后 $n$ （例如 $n=5$）个词都可以作为目标词，例如：

<table>
	<thead>
		<tr>
			<th>Context</th>
			<th>Target</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>orange</td>
			<td>juice</td>
		</tr>
		<tr>
			<td>orange</td>
			<td>glass</td>
		</tr>
		<tr>
			<td>orange</td>
			<td>want</td>
		</tr>
		<tr>
			<td>…</td>
			<td>…</td>
		</tr>
	</tbody>
</table>

然后使用这些词对构建一个监督学习模型。假设词汇表中有10000个单词，需要的词嵌入为300维，则网络的输入层有10000个节点，隐藏层有300个节点，输出层有10000个节点。我们所需要的嵌入矩阵也就是从输入层到隐藏层的权重矩阵。

输入层输入的是context的one-hot向量，即 $o_c$；输出层会经过softmax计算，得到 $\hat{y}$，然后使用 $\hat{y}$ 和 $y$（即target的one-hot向量 $o_t$）计算loss。

$$\mathcal{L}(\hat{y},y)=-\sum_{i=1}^{10000}y_i\text{log}\hat{y}_i$$

## 5. 负采样

假设同样的句子：I want a glass of orange juice to go along with my cereal. 从句子中选取一对context和target作为正样本，然后使用context，并从词典里随机选取其他词和context组成单词对，作为负样本。例如：

<table>
	<thead>
		<tr>
			<th>Context</th>
			<th>Target</th>
			<th>y</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>orange</td>
			<td>juice</td>
			<td>1</td>
		</tr>
		<tr>
			<td>orange</td>
			<td>king</td>
			<td>0</td>
		</tr>
		<tr>
			<td>orange</td>
			<td>book</td>
			<td>0</td>
		</tr>
		<tr>
			<td>orange</td>
			<td>the</td>
			<td>0</td>
		</tr>
		<tr>
			<td>orange</td>
			<td>of</td>
			<td>o</td>
		</tr>
		<tr>
			<td>…</td>
			<td>…</td>
			<td>…</td>
		</tr>
	</tbody>
</table>

这里需要注意的是，虽然of出现在orange附近，但是由于最初我们选取的是orange和juice作为正样本，所以我们认为orange和of是一个负样本。也就是说，即使从词典中随机选取的词刚好出现在context附近，我们也认为是一个负样本。

接下来通过这些样本来构建一个监督学习。在使用skip-gram模型的时候，从隐藏层到输出层会涉及到大量的参数，并且最后的softmax计算也非常耗时。在使用负采样的时候，输出层不再采用softmax，而是使用sigmoid函数。假如有1个正样本和5个负样本，则输出层一共有六个节点，相当于是六个逻辑回归问题。

## 6. GloVe

即Global Vectors for Word Representation，是另外一种词嵌入算法。

定义 $X_{ij}$ 为单词 $i$ 出现在单词 $j$ 的上下文中的次数。如果认定上下文为某个单词左右两个方向的 $n$ 个词范围，则 $X_{ij}=X_{ji}$。则我们需要：

$$\text{minimize}\sum_{i=1}^{10000}\sum_{j=1}^{10000}f(X_{ij})(\theta_i^Te_j+b_i-b_j'-\text{log}X_{ij})^2$$

其中 $f$ 是一个权重函数，例如当 $X_{ij}=0$ 的时候，需要 $f(X_{ij})=0$。

## 7. 情感分类

文本的情感分类是一个很常见的问题，例如对餐厅的评分：

<table>
	<thead>
		<tr>
			<th>x</th>
			<th>y</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>The dessert is excellent.</td>
			<td>★★★★☆</td>
		</tr>
		<tr>
			<td>Service was quite slow.</td>
			<td>★★☆☆☆</td>
		</tr>
		<tr>
			<td>Good for a quick meal, but nothing special.</td>
			<td>★★★☆☆</td>
		</tr>
		<tr>
			<td>Completely lacking in good taste, good service, and good ambience.</td>
			<td>★☆☆☆☆</td>
		</tr>
	</tbody>
</table>

一种方式就是对输入每个单词的嵌入向量求均值，然后通过softmax输出评分，如图所示：

![]({{site.baseurl}}/images/deeplearning/14-2.png)

不过这个算法也存在一个很大的问题，也就是不考虑词序，会把同一个评价中，正面评价的部分和负面评价的部分相中和。

更好的方式是使用一个RNN网络来进行计算。

## 8. 词嵌入消除偏见

机器学习的方式可能会被用于辅助制定社会决策等，因此在这个过程中，需要避免一些不应该出现的偏见，例如性别歧视，种族歧视等。

例如：Man:Woman as King:Queen 是合理的，但是 Man:Computer_Programmer as Woman:Homemaker 则有着一些偏见的意味，其它不合理的例子还有 Father:Doctor as Mother:Nurse 等。

也就是说，由于所选取的训练文本有可能带有一些社会偏见，因此词嵌入有可能会在一定程度上反映性别、种族、年龄、性取向等方面的偏见。

相关论文可以参考 [Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings](https://arxiv.org/abs/1607.06520).