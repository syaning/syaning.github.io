---
layout: post
title:  DeepLearning笔记(8)——ResNet
date:   2018-03-16 20:30:00 +0800
---

* TOC
{:toc}

## 1. Shortcut Connection

随着网络层次的越来越深，就容易出现梯度消失或者梯度爆炸的问题，因此网络层数无法很深。残差网络（Residual Network）的出现很好的解决了这个问题。相关论文可以参考[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)。

ResNet的主要思想是跳远连接（shortcut connection），其基本组成是残差块（building block）。残差块的结构如下：

![]({{site.baseurl}}/images/deeplearning/8-1.svg)

$$
\begin{array}{}
z^{[l+1]}=W^{[l+1]}a^{[l]}+b^{[l+1]} \\
a^{[l+1]}=g(z^{[l+1]}) \\
z^{[l+2]}=W^{[l+2]}a^{[l+1]}+b^{[l+2]} \\
a^{[l+2]}=g(z^{[l+2]}+a^{[l]})
\end{array}
$$

可以跳过一个或多个网络层，上图示例是跳过了一个网络层。通过一个个残差块堆叠在一起，可以训练更深的神经网络。

## 2. Identity Shortcut & Projection Shortcut

- Identity Shortcut：$a^{[l+2]}$ 和 $a^{[l]}$ 的维度一致，即 $a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$
- Projection Shortcut：$a^{[l+2]}$ 和 $a^{[l]}$ 的维度不一致，此时 $a^{[l+2]}=g(z^{[l+2]}+W_sa^{[l]})$

如下图所示，最右边即为ResNet34的结构。其中实线连接的为identity shortcut，虚线连接的为projection shortcut。

![]({{site.baseurl}}/images/deeplearning/8-2.png)

## 3. Bottleneck

在上面的ResNet34结构图中，可以看到，每个building block都是两层 $3\times3$ 的卷积。我们通过把每个 builiding block 替换成三层分别为 $1\times1,3\times3,1\times1$ 的卷积，即可以得到ResNet50。这就是Bottleneck结构，如图所示：

![]({{site.baseurl}}/images/deeplearning/8-3.png)

Bottleneck的主要思想是先使用 $1\times1$ 的卷积降低维度，最后再使用 $1\times1$ 的卷积来提升维度。这样做可以有效减少参数数量，从而构建更深的网络。 

例如上图右边的结构，如果使用两层的 $3\times3$ 的卷积，则参数数量为 $3\times3\times256\times256\times2=1179648$，而使用了bottleneck结构后，参数数量只有 $1\times1\times256\times64+3\times3\times64\times64+1\times1\times64\times256=69632$。