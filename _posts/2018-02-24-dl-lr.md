---
layout: post
title:  DeepLearning笔记(1)——逻辑回归
date:   2018-02-24 15:00:00 +0800
---

* TOC
{:toc}

## 1. 符号表示

假设有一批猫和非猫的图片，要判断是否是猫。假设图片都是彩色图片，尺寸为$64\times64$。

![]({{site.baseurl}}/images/deeplearning/1-1.png)

在训练的时候，将图片进行铺平展开，如下图所示：

<img src="{{site.baseurl}}/images/deeplearning/1-2.png" style="width:150px">

用$m$表示样本的个数，$n_x$表示特征数，则：

$$
\begin{array}{}
n_x = 64 \times 64 \times 3 = 12288 \\
x \in \mathbb{R}^{n_x} \\
y \in \{0, 1\} \\
X = \begin{bmatrix} \mid & & \mid \\ x^{(1)} & \dots & x^{(m)} \\ \mid & & \mid \\ \end{bmatrix} \in \mathbb{R}^{n_x \times m} \\
Y = \begin{bmatrix} y^{(1)} \dots y^{(m)} \end{bmatrix} \in \mathbb{R}^{1 \times m}
\end{array}
$$

## 2. sigmoid

对于二元分类的一个样本输入$(x, y)$，令$\hat{y}=P(y=1 \mid x)$。即$\hat{y}$是一个概率值。

令$w \in \mathbb{R}^{n_x}$，$b \in \mathbb{R}$，则$\hat{y} = \sigma(w^Tx+b)$，其中$\sigma$为sigmoid函数，$\sigma(z)=\frac{1}{1+e^{-z}}$。

![]({{site.baseurl}}/images/deeplearning/1-3.png)

## 3. 损失函数

对于一个训练样本，定义其损失函数为：

$$\mathcal{L}(\hat{y},y)=-(y\log\hat{y}+(1-y)\log(1-\hat{y}))$$

则：

- 当$y=1$时，$\mathcal{L}(\hat{y},y)=-\log\hat{y}$，为了使 $\mathcal{L}(\hat{y},y)$ 尽量小，需要 $\hat{y}$ 尽量大
- 当$y=0$时，$\mathcal{L}(\hat{y},y)=-\log(1-\hat{y})$，为了使 $\mathcal{L}(\hat{y},y)$ 尽量小，需要 $\hat{y}$ 尽量小

整体的损失函数为：

$$J(w,b)=\frac{1}{m}\sum_{i=1}^m\mathcal{L}(\hat{y}^{(i)},y^{(i)})=-\frac{1}{m}\sum_{i=1}^m[y^{(i)}\log\hat{y}^{(i)}+(1-y^{(i)})\log(1-\hat{y}^{(i)})]$$

## 4. 单个样本的梯度下降

对于一个样本，令：

$$
\begin{array}{}
z=w^Tx+b \\
a=\hat{y}=\sigma(z)=\frac{1}{1+e^{-z}} \\
\mathcal{L}(a,y)=\mathcal{L}(\hat{y},y)=-(y\log{a}+(1-y)\log(1-a))
\end{array}
$$

则：

$$
\begin{array}{}
\frac{\text{d}\mathcal{L}(a,y)}{\text{d}a}=-\frac{y}{a}+\frac{1-y}{1-a} \\
\frac{\text{d}a}{\text{d}z}=\frac{e^{-z}}{(1+e^{-z})^2}=a(1-a) \\
\frac{\text{d}z}{\text{d}w_j}=x_j \\
\frac{\text{d}z}{\text{d}b}=1
\end{array}
$$

因此：

$$
\begin{array}{}
\frac{\partial\mathcal{L}}{\partial w_j}=\frac{\text{d}\mathcal{L}}{\text{d}a}\frac{\text{d}a}{\text{d}z}\frac{\text{d}z}{\text{d}w_j}=(a-y)x_j \\
\frac{\partial\mathcal{L}}{\partial b}=\frac{\text{d}\mathcal{L}}{\text{d}a}\frac{\text{d}a}{\text{d}z}\frac{\text{d}z}{\text{d}b}=a-y
\end{array}
$$

## 5. 梯度下降

对于整体：

$$
\begin{array}{}
\frac{\partial}{\partial w_j}J(w,b)=\frac{1}{m}\sum_{i=1}^m\frac{\partial}{\partial w_j}\mathcal{L}(a^{(i)},y^{(i)}) \\
\frac{\partial}{\partial b}J(w,b)=\frac{1}{m}\sum_{i=1}^m\frac{\partial}{\partial b}\mathcal{L}(a^{(i)},y^{(i)}) \\
w_j := w_j-\alpha\frac{\partial}{\partial w_j}J(w,b) \\
b := b-\alpha\frac{\partial}{\partial b}J(w,b)
\end{array}
$$

其中 $\alpha$ 为学习速率。

写成向量化的形式为：

$$
\begin{array}{}
Z = w^TX+b \\
A = \sigma(Z)=\sigma(w^TX+b) \\
dZ = A-Y \\
dw = \frac{1}{m}X\text{d}Z^T=\frac{1}{m}X(A-Y)^T \\
db = \frac{1}{m}\text{sum}(dZ)=\frac{1}{m}\text{sum}(A-Y) \\
w := w-\alpha\text{d}w \\
b := b-\alpha\text{d}b
\end{array}
$$