---
layout: post
title:  DeepLearning笔记(15)——序列模型
date:   2018-05-13 21:50:00 +0800
---

* TOC
{:toc}

## 1. Sequence to Sequence

例如语言翻译：

- 输入：Jane visite l'Afrique en septembre. 即 $x=(x^{\langle 1 \rangle},x^{\langle 2 \rangle},x^{\langle 3 \rangle},x^{\langle 4 \rangle},x^{\langle 5 \rangle})$
- 输出：Jane is visiting Africa in September. 即 $y=(y^{\langle 1 \rangle},y^{\langle 2 \rangle},y^{\langle 3 \rangle},y^{\langle 4 \rangle},y^{\langle 5 \rangle},y^{\langle 6 \rangle})$

为了实现这个过程，首先实现一个编码网络（Encoder Network），将输入的句子编码为一个向量；然后通过一个解码网络（Decoder Network），来输出翻译后的句子。如图所示：

![]({{site.baseurl}}/images/deeplearning/15-1.png)

相关论文参考：

- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)

## 2. Image Captioning

即看图说话，给一张图片，然后输出对应的文字描述。例如下面的图片：

![]({{site.baseurl}}/images/deeplearning/15-2.jpg)

文字描述为：A cat sitting on a chair.

实现过程与上面翻译的例子相似。只不过首先是通过CNN作为编码网络来得到一个向量，然后通过类似的解码网络输出描述文字。

## 3. Beam Search

### （1）算法

通过前文讲解，我们已经知道在机器翻译工程中，使用了一个编码网络和一个解码网络。如图所示：

![]({{site.baseurl}}/images/deeplearning/15-3.png)

这个网络结构与之前的语言模型非常相似，只不过语言模型只有解码网络这部分，并且其输入是一个零向量。因此可以任务机器翻译使用的是一个条件语言模型，即在给定条件（输入的句子）的情况下的语言模型。因此最终计算的其实是：

$$P(y^{\langle 1 \rangle},\cdots,y^{\langle T_y \rangle} \mid x^{\langle 1 \rangle},\cdots,x^{\langle T_x \rangle})$$

给定一句原文，翻译的结果可能有多个，例如原文为：Jane visite l'Afrique en septembre. 翻译结果可能为：

- Jane is visiting Africa in September.
- Jane is going to be visiting Africa in September.
- In September, Jane will visit Africa.
- Her African friend welcomed Jane in September.

那么如何选择最合适的译文呢？其实就是求：

$$\underset{y^{\langle 1 \rangle},\cdots,y^{\langle T_y \rangle}}{\text{argmax}} \space P(y^{\langle 1 \rangle},\cdots,y^{\langle T_y \rangle} \mid x)$$

因此需要有一个算法来最大化这个条件概率。在这个场景中，常使用的是 beam search 算法，算法过程如下：

首先选择译文的第一个单词 $y^{\langle 1 \rangle}$，假如词典里一共有10000个单词，我们设置了 $B=3$，那么就需要从词典中选取三个最有可能是译文开头的单词，即选取最大的三个 $P(y^{\langle 1 \rangle} \mid x)$。 例如选取“jane”，“in”，“september”作为三个候选项。

第一个单词选定之后，就要开始选择第二个单词。由于第一个单词有三个选择，词典一共有10000个单词，因此此时一共有30000个组合。我们需要选取最大的三个 $P(y^{\langle 1 \rangle},y^{\langle 2 \rangle} \mid x)$，在这里需要注意的是 $P(y^{\langle 1 \rangle},y^{\langle 2 \rangle} \mid x)=P(y^{\langle 1 \rangle} \mid x)P(y^{\langle 2 \rangle} \mid x,y^{\langle 1 \rangle})$。例如到这一步，我们选取的译文前两个单词为“in september”，“jane is”，“jane visits”。

在选取了前两个单词的基础上，再去选择第三个单词。依次类推。如果设置了 $B=10$，那么每一步就要选出十个候选项。那么如何选择合适的 $B$ 呢？

- $B$ 越大，可选的结果就越多，越有可能得到最好的结果，但是计算量也随着增大
- $B$ 越小，可选的结果就越少，最终的结果可能不是最好的，但是算法运行会快一些

在实践中，根据应用不同，会选择不同大小的 $B$。

### （2）Length Normalization

在实际使用过程中，会对目标函数做一些调整。考虑到 beam search 最终求的是：

$$\underset{y^{\langle 1 \rangle},\cdots,y^{\langle T_y \rangle}}{\text{argmax}} \space P(y^{\langle 1 \rangle},\cdots,y^{\langle T_y \rangle} \mid x)$$

即：

$$\underset{y}{\text{argmax}}\prod_{t=1}^{T_y}P(y^{\langle t \rangle} \mid x,y^{\langle 1 \rangle},\cdots,y^{\langle t-1 \rangle})$$

但是由于 $P(y^{\langle t \rangle} \mid x,y^{\langle 1 \rangle},\cdots,y^{\langle t-1 \rangle})$ 都是介于0到1之间的小数，因此相乘之后会导致乘积非常小，甚至会导致underflow，在这种情况下，需要作出调整，将所求目标提调整为：

$$\underset{y}{\text{argmax}}\sum_{t=1}^{T_y}\text{log}P(y^{\langle t \rangle} \mid x,y^{\langle 1 \rangle},\cdots,y^{\langle t-1 \rangle})$$

但是这样依然存在着一个缺点，即由于 $P(y^{\langle t \rangle} \mid x,y^{\langle 1 \rangle},\cdots,y^{\langle t-1 \rangle})$ 都是介于0到1之间的小数，也就是说 $\text{log}P(y^{\langle t \rangle} \mid x,y^{\langle 1 \rangle},\cdots,y^{\langle t-1 \rangle})$ 都是负数，因此最终的结果是倾向于选择简短的翻译结果（因为翻译结果越长，累加的负数越多，目标值就越小）。为了避免这种情况，需要调整目标函数为：

$$\underset{y}{\text{argmax}}\frac{1}{T_y^\alpha}\sum_{t=1}^{T_y}\text{log}P(y^{\langle t \rangle} \mid x,y^{\langle 1 \rangle},\cdots,y^{\langle t-1 \rangle})$$

其中 $\alpha$ 是一个超参数，介于0到1之间：

- 当 $\alpha=0$ 时，相当于没有做归一化处理
- 当 $\alpha=1$ 时，相当于用译文长度来做归一化
- 大多数情况下，$0<\alpha<1$

### （3）误差分析

Beam Search 是一种近似搜索算法（区别于BFS、DFS这样的精确搜索），或者说是启发式搜索算法，它并不总是输出最好的结果。因此需要通过误差分析来判读是RNN模型（编码和解码）导致的误差还是由于Beam Search算法导致的误差。

例如下面的例子（机器翻译效果不如人工翻译）：

- 原文：Jane visite l'Afrique en septembre.
- 人工翻译：Jane visits Africa in September. ($y^{*}$)
- 机器翻译：Jane visited Africa last September. ($\hat{y}$)

通过计算 $P(y^{*} \mid x)$ 和 $P(\hat{y} \mid x)$：

- 如果 $P(y^{*} \mid x)>P(\hat{y} \mid x)$，则说明 Beam Search 算法没有能够选出最好的结果，因此是 Beam Search 算法出了问题
- 如果 $P(y^{*} \mid x) \leq P(\hat{y} \mid x)$，则说明 RNN 对差的翻译给了更好的概率值，因此是 RNN 模型出了问题

如果对目标函数应用了长度归一化，那么应当将应用了长度归一化的值进行比较。

在实际过程中，会对多个样本进行误差分析，然后统计 RNN 和 Beam Search 造成的错误比例，然后对出错更多的那部分进行优化。

### （4）BLEU得分

另一个问题是，在翻译过程中，可能会有多个同样好的结果，那么应该如何衡量准确性呢？例如下面的例子：

- 原文：Le chat est sur le tapis.
- 参考译文1：The cat is on the mat.
- 参考译文2：There is a cat on the mat.
- 机器翻译1：the the the the the the the.
- 机器翻译2：The cat the cat on the mat.

BLEU的理念是，观察机器翻译的结果，看生成的词是否出现在至少一个参考译文中。BLEU需要首先计算：

$$P_n=\frac{\sum_{ngram\in\hat{y}}count_{clip}(ngram)}{\sum_{ngram\in\hat{y}}count(ngram)}$$

其中：

- $ngram$ 表示译文中的 $n$ 元组，例如对于机器翻译2：
    - $n=1$: the, cat, the, cat, on, the, mat
    - $n=2$: the cat, cat the, the cat, cat on, on the, the mat
- $count(ngram)$ 表示 $ngram$ 在译文中出现的次数
- $count_{clip}(ngram)$ 表示 $ngram$ 在参考译文中出现的最大次数

例如对于参考译文1，$P_1=\frac{2}{7}$；对于参考译文2，$P_2=\frac{4}{6}$，计算如下：

<table>
	<thead>
		<tr>
			<th>$ngram$</th>
			<th>$count(ngram)$</th>
			<th>$count_{clip}(ngram)$</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>the cat</td>
			<td>2</td>
			<td>1</td>
		</tr>
		<tr>
			<td>cat the</td>
			<td>1</td>
			<td>0</td>
		</tr>
		<tr>
			<td>cat on</td>
			<td>1</td>
			<td>1</td>
		</tr>
		<tr>
			<td>on the</td>
			<td>1</td>
			<td>1</td>
		</tr>
		<tr>
			<td>the mat</td>
			<td>1</td>
			<td>1</td>
		</tr>
	</tbody>
</table>

最终的BLEU得分计算为：

$$BPe^{\frac{1}{4}\sum_{n=1}^{4}P_n}$$

其中 $BP$ 表示 brevity penality，即避免翻译结果太短从而导致BLEU得分偏高，其取值为：

$$
BP=\begin{cases}{}
1 & \text{if MT_output_length>reference_output_length} \\
e^{1-\frac{\text{MT_output_length}}{\text{reference_output_length}}} & \text{otherwise}
\end{cases}
$$

相关论文参考 [BLEU: a Method for Automatic Evaluation of Machine Translation](https://www.aclweb.org/anthology/P02-1040.pdf).