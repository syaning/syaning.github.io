---
layout: post
title:  DeepLearning笔记(2)——神经网络
date:   2018-02-24 18:00:00 +0800
---

* TOC
{:toc}

## 1. 前向传播

![]({{site.baseurl}}/images/deeplearning/2-1.svg)

如上图，左边为一个简单的神经网络结构，右边为每一个神经单元的计算过程。

对于一个样本：

$$
\begin{array}{}
a^{[0]}=x \\
z^{[1]}=W^{[1]}a^{[0]}+b^{[1]} \\
a^{[1]}=\sigma(z^{[1]}) \\
z^{[2]}=W^{[2]}a^{[1]}+b^{[2]} \\
a^{[2]}=\sigma(z^{[2]}) \\
\end{array}
$$

其中：

$$
\begin{array}{}
W^{[1]} \in \mathbb{R}^{4\times3} \\
b^{[1]} \in \mathbb{R}^{4\times1} \\
W^{[2]} \in \mathbb{R}^{1\times4} \\
b^{[2]} \in \mathbb{R}^{1\times1} \\
\end{array}
$$

一般来说，令：

- $L$ 表示神经网络层数（不包含输入层），输入层看作是第0层
- $n^{[l]}$ 表示第 $l$ 层的节点数量
- $g^{[l]}$ 表示第 $l$ 层的激活函数

则对于多个样本向量化：

$$
\begin{array}{}
X \in \mathbb{R}^{n^{[0]} \times m} \\
W^{[l]} \in \mathbb{R}^{n^{[l]} \times n^{[l-1]}} \\
b^{[l]} \in \mathbb{R}^{n^{[l]}\times1} \\
A^{[0]}=X \\
Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]} \\
A^{[l]}=g^{[l]}(Z^{[l]})
\end{array}
$$

## 2. 激活函数

- 不同的层可以使用不同的激活函数
- 激活函数是非线性的


常用的激活函数有：

$$
\begin{array}{}
\text{sigmoid}:&a=g(z)=\frac{1}{1+e^{-z}} \\
\text{tanh}:&a=g(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}} \\
\text{ReLU}:&a=g(z)=\text{max}(0,z) \\
\text{leaky ReLU}:&a=g(z)=\text{max}(0.01z,z)
\end{array}
$$

![]({{site.baseurl}}/images/deeplearning/2-2.png)

它们的导数分别为：

$$
\begin{array}{}
\text{sigmoid}:&g'(z)=a(1-a) \\
\text{tanh}:&g'(z)=1-a^2 \\
\text{ReLU}:&g'(z)=\begin{cases}0,&\text{if}\ z<0\\1,&\text{if}\ z>0 \end{cases} \\
\text{leaky ReLU}:&g'(z)=\begin{cases}0.01,&\text{if}\ z<0\\1,&\text{if}\ z>0 \end{cases}
\end{array}
$$

## 3. 反向传播

在前向传播过程中，针对每一层，输入为 $A^{[l-1]}$，输出为 $A^{[l]}$：

$$
\begin{array}{}
Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]} \\
A^{[l]}=g^{[l]}(Z^{[l]})
\end{array}
$$

在反向传播过程中，针对每一层，输入为 $\text{d}A^{[l]}$，输出为 $\text{d}A^{[l-1]}$，$\text{d}W^{[l]}$，$\text{d}b^{[l]}$：

$$
\begin{array}{}
\text{d}Z^{[l]}=\text{d}A^{[l]}*g^{[l]}{'}(Z^{[l]}) \\
dW^{[l]}=\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\
db^{[l]}=\frac{1}{m} \sum_{i = 1}^{m} dZ^{[l](i)} \\
dA^{[l-1]}=W^{[l] T} dZ^{[l]}
\end{array}
$$

## 4. 参数初始化

在逻辑回归中，参数 $w$ 和 $b$ 可以都初始化为0，但是在神经网络中，如果这样做的话，则每一层所有的神经单元都是在做着同样的计算。因此需要进行随机初始化。

初始化的参数值通常会很小。如果比较大，在使用了sigmoid或者tanh激活函数的时候，计算值就容易落到函数值较大的区域，从而梯度很小，导致反向传播过程进行的很慢。

## 5. 参数和超参数

参数：

- $W$
- $b$

超参数：

- 学习速率 $\alpha$
- 梯度下降迭代次数 $\text{iterations}$
- 神经网络层数 $L$
- 每一层的节点数 $n^{[l]}$
- 激活函数类型 $g(z)$
- ……

超参数决定了最终训练好的参数 $W$ 和 $b$ 的值，即超参数是控制参数的参数。