import{_ as e,c as t,ah as r,o as s}from"./chunks/framework.B8QGmHkS.js";const i="/assets/memory.C5l9k51k.webp",l="/assets/memory-buffer.B-AYnIVU.webp",o="/assets/memory-abstract.DRYgDVov.webp",n="/assets/memory-compare.5W0mVPXa.webp",p="/assets/react.DqoUsuKj.webp",m="/assets/semantic-search-dense-retrieval.C6-dnyh8.webp",c="/assets/semantic-search-reranking.DDiKbivE.webp",h="/assets/semantic-search-rag.CDM2psti.webp",d="/assets/vit.BKAqqHO_.webp",u="/assets/multimodal-1.BYxU0XAf.webp",_="/assets/multimodal-2.D7qH8EBG.webp",b="/assets/multimodal-3.C9s-PaOT.webp",R=JSON.parse('{"title":"图解大模型读书笔记——记忆、Agent、RAG、多模态","description":"","frontmatter":{"layout":"post","title":"图解大模型读书笔记——记忆、Agent、RAG、多模态","date":"2025-12-13 15:00:00 +0800","tags":"LLM"},"headers":[],"relativePath":"tech/posts/2025/2025-12-13-memory-rag-multimodal.md","filePath":"tech/posts/2025/2025-12-13-memory-rag-multimodal.md"}'),g={name:"tech/posts/2025/2025-12-13-memory-rag-multimodal.md"};function f(q,a,P,A,L,k){return s(),t("div",null,a[0]||(a[0]=[r('<h2 id="记忆" tabindex="-1">记忆 <a class="header-anchor" href="#记忆" aria-label="Permalink to &quot;记忆&quot;">​</a></h2><p>直接使用未经定制的LLM时，系统默认不具备对话记忆能力。即使在前序提示词中告知模型用户的姓名等信息，模型也无法在后续对话中记住这些信息。</p><p><img src="'+i+'" alt=""></p><p>两种广泛应用于对话记忆保持的方法：</p><ul><li>对话缓冲区</li><li>对话摘要</li></ul><h3 id="对话缓冲区" tabindex="-1">对话缓冲区 <a class="header-anchor" href="#对话缓冲区" aria-label="Permalink to &quot;对话缓冲区&quot;">​</a></h3><p><img src="'+l+'" alt=""></p><ul><li>随着对话轮次增加，输入提示词的文本长度会持续增长，最终可能超出模型的词元限制。</li><li>控制上下文窗口的有效策略是仅保留最近k轮对话记录。</li></ul><h3 id="对话摘要" tabindex="-1">对话摘要 <a class="header-anchor" href="#对话摘要" aria-label="Permalink to &quot;对话摘要&quot;">​</a></h3><ul><li>对完整对话记录进行摘要提炼，保留核心信息。</li><li>提炼摘要的过程由另一个LLM完成：该模型接收完整对话历史作为输入，并负责生成简明的摘要。使用外部LLM的显著优势在于，对话系统无须局限于单一模型。</li></ul><p><img src="'+o+'" alt=""></p><h3 id="对比" tabindex="-1">对比 <a class="header-anchor" href="#对比" aria-label="Permalink to &quot;对比&quot;">​</a></h3><p><img src="'+n+'" alt=""></p><h2 id="agent" tabindex="-1">Agent <a class="header-anchor" href="#agent" aria-label="Permalink to &quot;Agent&quot;">​</a></h2><p>能够自主规划行动及其序列的系统被称为智能体(agent)，其核心在于利用语言模型自主制定行动决策。Agent 通过两大核心组件实现功能扩展：</p><ul><li>工具(tool)：赋予智能体完成自身无法独立处理的任务的能力。</li><li>智能体类型(agent type)：规划行动及工具使用策略的决策框架。</li></ul><p>ReAct的精妙之处在于建立了推理与行动的动态反馈机制：推理指导行动决策，行动结果“反哺”推理进程。在具体实现中，系统通过以下三阶段的循环迭代完成认知闭环：</p><ul><li>思考(thought)</li><li>行动(action)</li><li>观察(observation)</li></ul><p><img src="'+p+'" alt=""></p><h2 id="语义搜索与rag" tabindex="-1">语义搜索与RAG <a class="header-anchor" href="#语义搜索与rag" aria-label="Permalink to &quot;语义搜索与RAG&quot;">​</a></h2><p>语义搜索(semantic search)核心在于通过语义理解而非简单的关键词匹配来实现精准检索。</p><p>尽管模型能够流畅自信地输出答案，但其内容在准确性和时效性方面仍存在不足（幻觉）。解决该问题的主要方法之一，便是构建能够实时检索相关信息并输入LLM的系统，从而生成有事实依据的答案。这种被称为RAG（检索增强生成）的技术，现已成为LLM最受瞩目的应用场景。</p><p>优化语言模型在搜索领域的应用，当前主流技术可分为三大类：</p><ul><li>稠密检索(dense retrieval)</li><li>重排序(reranking)</li><li>RAG</li></ul><h3 id="稠密检索" tabindex="-1">稠密检索 <a class="header-anchor" href="#稠密检索" aria-label="Permalink to &quot;稠密检索&quot;">​</a></h3><p>基于文本嵌入，将搜索问题转化为查询向量与文档向量的最近邻匹配过程。如图所示：接收搜索请求后，系统在文档库中进行向量比对，最终输出相关性最高的结果集合。</p><p><img src="'+m+'" alt=""></p><h3 id="重排序" tabindex="-1">重排序 <a class="header-anchor" href="#重排序" aria-label="Permalink to &quot;重排序&quot;">​</a></h3><p>搜索系统多采用多阶段处理流程。重排序模型作为其中关键环节，负责对初步检索结果进行相关性评分，并据此优化排序。</p><p><img src="'+c+'" alt=""></p><h3 id="rag" tabindex="-1">RAG <a class="header-anchor" href="#rag" aria-label="Permalink to &quot;RAG&quot;">​</a></h3><p>生成式搜索属于广义的RAG系统范畴。这类系统通过整合检索机制来增强文本生成功能，可有效抑制幻觉现象、提升事实准确性，并使模型输出与特定数据集保持逻辑一致性。</p><p><img src="'+h+'" alt=""></p><h2 id="多模态llm" tabindex="-1">多模态LLM <a class="header-anchor" href="#多模态llm" aria-label="Permalink to &quot;多模态LLM&quot;">​</a></h2><h3 id="视觉transformer" tabindex="-1">视觉Transformer <a class="header-anchor" href="#视觉transformer" aria-label="Permalink to &quot;视觉Transformer&quot;">​</a></h3><p>视觉Transformer(Vision Transformer，ViT)的实现依赖Transformer架构的一个重要组件——编码器。不同于将文本分割为词元，它将原始图像切割为规则排列的图像块，进而实现特征提取。</p><p>但需注意，无法像文本词元那样直接赋予图像块固定的ID——由于图像的多样性，图像块在不同场景中的重复概率远低于文本词元。 为解决这一问题，系统会对图像块实施线性嵌入操作，将其转换为数值化的嵌入向量。当嵌入向量进入编码器后，视觉与文本模态的处理路径完全相同。</p><p><img src="'+d+'" alt=""></p><h3 id="多模态嵌入模型" tabindex="-1">多模态嵌入模型 <a class="header-anchor" href="#多模态嵌入模型" aria-label="Permalink to &quot;多模态嵌入模型&quot;">​</a></h3><p>多模态嵌入模型可在同一向量空间中为不同模态生成嵌入向量。</p><p><img src="'+u+'" alt=""></p><p>得益于共享向量空间的特性，我们可以直接比较不同模态的语义表示。</p><p><img src="'+_+'" alt=""></p><p>在众多多模态嵌入模型中，对比语言-图像预训练(Contrastive Language-Image Pre-training，CLIP)模型以其卓越的性能和广泛的适用性成为当前最主流的解决方案。</p><p>CLIP的实现逻辑相当简洁。设想存在一个包含数百万张图像及其对应描述文本的训练数据集，该数据集为构建跨模态对齐提供了基础。基于此数据集，可为每一组图像和描述文本创建两个向量表示。CLIP采用双编码器架构实现这一点：文本编码器处理描述文本，生成语义嵌入；图像编码器提取视觉特征，生成图像嵌入。如图所示，经过联合训练后，配对的图文数据将在向量空间中获得高度对齐的嵌入向量表示。</p><p><img src="'+b+'" alt=""></p>',46)]))}const T=e(g,[["render",f]]);export{R as __pageData,T as default};
